# Truncated EfficientNet for Tuberculosis Classification

## ðŸ“– Description

This repository contains the full implementation and experimental analysis from my Master's thesis, which investigates **truncated versions of EfficientNet-B0** for binary classification of Tuberculosis (TB) in Chest X-Rays (CXRs).

The goal: reduce model size by strategically removing blocks from EfficientNet-B0â€”**without sacrificing diagnostic accuracy**. This lightweight approach is ideal for **real-world deployment in resource-constrained healthcare settings**.

---

### ðŸ—ï¸ Methodology Overview

The study evaluates five models: the full EfficientNet-B0 and four truncated variants (removing 1 to 4 blocks). The core idea is to **systematically remove later-stage MBConv blocks**, replacing them with a lightweight `Conv1x1` layer to preserve feature flow.

#### ðŸ”¹ Full EfficientNet-B0 Architecture

The baseline model includes 10 blocks, starting with a `Conv3x3` stem, followed by MBConv1 and MBConv6 layers, and ending with a high-capacity `Conv1x1` and fully connected classifier.

| Block | Operator         | Resolution     | # Channels | # Layers | Pretrained |
|-------|------------------|----------------|------------|-----------|------------|
| 1     | Conv3x3          | 224 Ã— 224      | 32         | 1         | âœ…          |
| 2     | MBConv1, k3x3    | 112 Ã— 112      | 16         | 1         | âœ…          |
| 3     | MBConv6, k3x3    | 112 Ã— 112      | 24         | 2         | âœ…          |
| 4     | MBConv6, k5x5    | 56 Ã— 56        | 40         | 2         | âœ…          |
| 5     | MBConv6, k3x3    | 28 Ã— 28        | 80         | 3         | âœ…          |
| 6     | MBConv6, k5x5    | 14 Ã— 14        | 112        | 3         | âœ…          |
| 7     | MBConv6, k5x5    | 14 Ã— 14        | 192        | 4         | âœ…          |
| 8     | MBConv6, k3x3    | 7 Ã— 7          | 320        | 1         | âœ…          |
| 9     | Conv1x1          | 7 Ã— 7          | 1280       | 1         | âœ…          |
| 10    | Pooling & FC     | 7 Ã— 7          | 1280       | 1         | âŒ          |

---

#### ðŸ”¹ Proposed B0(-3) Truncated Architecture

The **B0(-3)** model removes blocks **6, 7, and 8**, reducing complexity while preserving core feature pathways. A new untrained `Conv1x1` block with 112 output channels bridges the remaining network to the classifier.

| Block | Operator         | Resolution     | # Channels | # Layers | Pretrained |
|-------|------------------|----------------|------------|-----------|------------|
| 1     | Conv3x3          | 224 Ã— 224      | 32         | 1         | âœ…          |
| 2     | MBConv1, k3x3    | 112 Ã— 112      | 16         | 1         | âœ…          |
| 3     | MBConv6, k3x3    | 112 Ã— 112      | 24         | 2         | âœ…          |
| 4     | MBConv6, k5x5    | 56 Ã— 56        | 40         | 2         | âœ…          |
| 5     | MBConv6, k3x3    | 28 Ã— 28        | 80         | 3         | âœ…          |
| 6     | Conv1x1          | 7 Ã— 7          | 112        | 1         | âŒ          |
| 7     | Pooling & FC     | 7 Ã— 7          | 112        | 1         | âŒ          |

> ðŸ” The new Conv1x1 in block 6 ensures smooth dimensional transition while slashing the model to just **~308K parameters**â€”over **13Ã— smaller** than the original B0.

---

### ðŸ§ª Experimental Setup

- **Datasets**: 
  - Training: Balanced Kaggle dataset (3,500 TB + 3,500 Normal CXRs)
  - External Testing: Mendeley TB datasets from Pakistan, annotated with Urdu text (âˆ¼4,800 images)
- **Training Strategy**:
  - 40 epochs
  - Adam optimizer with LR scheduler
  - No data augmentation (but supported in codebase)
- **Evaluation**:
  - Internal test set (10% of Kaggle data)
  - External generalization on Mendeley data
  - **500 bootstrap iterations** for confidence intervals on external test set

---

### âœ… Core Insight

Despite its simplicity, **B0(-3)** generalizes extremely well, achieving:
- **97.38% external test accuracy**
- **98.96% sensitivity**
- **95.68% specificity**

These results meet **WHO guidelines for TB diagnostics** and outperform some existing full-size modelsâ€”all while being lightweight enough for edge deployment.


Key contributions:
- ðŸ”¬ Systematic truncation of EfficientNet-B0 (from -1 to -4 blocks)
- ðŸ“Š Rigorous evaluation on internal (Kaggle) and external (Mendeley) datasets
- ðŸ“ˆ Bootstrap analysis with 95% confidence intervals
- âš–ï¸ Trade-off analysis between accuracy and model efficiency

---

## ðŸ† Key Findings

- âœ… **100% accuracy** on internal Kaggle test set with all models
- ðŸŒ **97.4% external accuracy** with B0(-3), including:
  - Sensitivity: **98.96%**
  - Specificity: **95.68%**
- âš¡ B0(-3) uses only **308K parameters**, compared to **4.1M** in the full B0
- ðŸ“‰ 63Ã— smaller than prior DenseNet-201â€“based approaches
- ðŸš€ Real-world potential for clinical use in low-resource settings

---

## ðŸ“ Repository Structure

```
ðŸ“ cxr_thesis/
â”œâ”€â”€ custom_lib/
â”‚   â”œâ”€â”€ data_prep.py
â”‚   â”œâ”€â”€ eval_tools.py
â”‚   â””â”€â”€ custom_models/
â”‚       â”œâ”€â”€ truncated_b0.py, truncated_b0_leaky.py, etc.
â”‚
â”œâ”€â”€ run_model.py                  # Training script (full + truncated models)
â”œâ”€â”€ run_model.ipynb              # Jupyter training variant
â”œâ”€â”€ run_experiments.sh           # Batch experiment runner
â”œâ”€â”€ run_bootstraps.ipynb         # Bootstrap CI evaluation
â”œâ”€â”€ explore_model.ipynb          # Early exploration / architecture tests
â”œâ”€â”€ replot_training_loss.ipynb   # Plot regeneration for report
â”œâ”€â”€ results/                     # Saved metrics, checkpoints
â”œâ”€â”€ external_bootstrap_results/  # External test bootstrap metrics
â”œâ”€â”€ paper_figs/                  # Final figure exports for thesis
â”œâ”€â”€ results_figs.ipynb           # Plot generation scripts
â”œâ”€â”€ plots_presentation.pptx      # Presentation slides
â”œâ”€â”€ requirements.txt             # Dependency list
â””â”€â”€ thesis.pdf                   # Final paper
```

---

## ðŸ› ï¸ Requirements

- Python 3.8+
- PyTorch â‰¥ 1.10
- torchvision
- scikit-learn
- pandas, matplotlib, seaborn

```bash
pip install -r requirements.txt
```

---

## ðŸš€ Usage

### 1. Prepare Data
Download and organize the datasets:
- [Kaggle TB Dataset](https://www.kaggle.com/datasets/tawsifurrahman/tuberculosis-tb-chest-xray-dataset)
- [Mendeley Pakistan TB Dataset](https://data.mendeley.com/datasets/jctsfj2sfn/1)

Place the images into a `data/` directory using the format expected by `data_prep.py`.

---

### 2. Train Model
```bash
python run_model.py --model b0_minus_3
```

### 3. Evaluate on External Data
```bash
python run_model.py --model b0_minus_3 --eval_only --external
```

### 4. Bootstrap Confidence Intervals
```bash
jupyter notebook run_bootstraps.ipynb
```

---

## ðŸ“Š Results Summary

| Model     | Params | Internal Acc | External Acc | Sensitivity | Specificity |
|-----------|--------|--------------|--------------|-------------|-------------|
| B0(-0)    | 4.1M   | 100%         | 97.26%       | 98.72%      | 95.68%      |
| B0(-3) ðŸ”¥ | 308K   | 100%         | 97.38%       | 98.96%      | 95.68%      |

> ðŸ”¥ B0(-3) is 13Ã— smaller than B0(-0), with overlapping performance and better efficiency.



